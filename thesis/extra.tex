
%\begin{observation}
%\label{obs:p-q}
%Every vector parallel to the face $\splx_U$ is parallel to a vector of the form $\p-\q$ for $\p,\q\in\splx_U$. 
%\end{observation}



%\TODO Give intuition for what's about to happen. And give figures.
%
%\begin{lemma}
% $\spn(\{\sv_1-\sv_j\}_{j\in U})$ is parallel to $\splx_U$. 
%\end{lemma}
%Fix $i\in[n]$. By virtue of the fact that the vertex vectors are affinely independent, it follows that 
%\[\dim(\spn(\{\sv_1-\sv_j\}_{j\geq 2,j\neq i})=n-2,\]
%meaning the set of vectors $\{\sv_1-\sv_j\}_{j\neq i}$ forms a hyperplane which passes through the origin. Let $\w$ be the normal direction of this hyperplane, so that $\la \w,\v\ra=0$ for all \begin{equation}
%\label{eq:v_in_span}
%    \v=\sum_{j\geq 2,j\neq i}\alpha_j(\sv_1-\sv_j)\in\spn_\R(\{\sv_1-\sv_j\}_{j\geq 2,j\neq i})
%\end{equation} it holds that  $\la \w,\v\ra=0$. Let $\p,\q\in\splx_\ic$, so that $\p-\q$ is parallel so $\splx_\ic$. Let $\x$ and $\y$ be the barycentric coordinates of $\p$ and $\q$. Observe that 
%\begin{align*}
%    \p-\q&= \Sv(\x-\y) 
%    = \sum_{j\neq i} (x(i) - y(i))\sv_j \\
%    &= \bigg(1-\sum_{\ell}x(\ell) - 1 + \sum_\ell y(\ell)\bigg)\sv_1 + \sum_{j\geq 2;j\neq i}(x(j)-y(j))\sv_j \\
%    &= \sum_{j\geq 2,j\neq i} (y(j)-x(j))(\sv_1-\sv_j). 
%\end{align*}
%The inequality on the second line follows from the fact that $\x$ and $\y$ are barycentric coordinates and hence sum to one. Taking $\alpha_j = y(j)=x(j)$ in \eqref{eq:v_in_span} shows that $\la \w,\p-\q\ra =0$. Combined with Claim \ref{claim:p-q}, this demonstrates that $\spn_\R(\{\sv_1-\sv_j\}_{j\neq i})$ is parallel to $\splx_\ic$. 
%
%\begin{lemma}
%$\splx_U\subset \spn(\{\sv_1-\sv_j\}_{j\in U}) + \cent(\splx_U)$. 
%\end{lemma}
%
%For each $i\in[n]$ we therefore define the \emph{hyperplane associated with the facet $\splx_\ic$} as 
%\begin{equation}
%    \H_\ic \equiv \spn(\{\sv_1-\sv_j\}_{j\neq i,j>1}) + \cent(\splx_\ic).
%\end{equation}
%In other words, $\H_\ic$ is the affine space of dimension $n-2$ in which $\splx_\ic$ lies.
%
%\begin{lemma}
%For any proper subset $U\subsetneq[n]$, 
%$\dim(\spn_\R(\{\sv_j\}_{j\in U}))=|U|$. 
%\end{lemma}
%\begin{proof}
%It's immediate that 
%\[|U|\geq \dim(\spn(\{\sv_j\}_{j\in U})) \geq \dim(\spn_\R(\{\sv_1-\sv_j\}_{j\in U}))=|U|-1,\]
%by linear independence of the set $\{\sv_1-\sv_j\}$. Therefore, to demonstrate that the dimension is indeed $|U|$, it suffices to find a point $\x\in\spn(\{\sv_j\}_{j\in U})\setminus \spn_\R(\{\sv_1-\sv_j\}_{j\in U})$. 
%\end{proof}
%
%\note{Unclear where this stuff goes/whether it's needed}
%\begin{lemma}
%\label{lem:svi_independent}
%Let $G=(V,E)$ be a graph, and let $U\subsetneq V$ (i.e., $|U|<n)$. Then 
%\[\dim(\spn_\R(\{\sv_i\}_{i\in U}))=\dim(\spn_\R(\{\sv_i^+\}_{i\in U}))=|U|.\]
%\end{lemma}
%
%\begin{lemma}
%\label{lem:face_shift}
%Let $U\subsetneq V$. For each $i\in U$ set $\bgam_i = \sv_i - c_U$ where $c_U=c(\splx_U)$ is the centroid of the face $\splx_U$. Then 
%\[\dim(\spn_\R(\{\bgam_i\}_{i\in U}))=|U|-1,\]
%and every vector in $\spn(\{\bgam_i\}_{i\in U}))^\perp$, i.e., the orthogonal complement of $ \spn(\{\bgam_i\}_{i\in U}))$, is perpendicular to $\splx_{U}$. 
%\end{lemma}
%\begin{proof}
%For notational convenience, put $\X=\spn(\{\bgam_i\}_{i\in U})$. We begin by considering $\dim(\X)$. Notice that 
%\[\sum_{i\in U}\bgam_i = \sum_{i\in U}\bigg(\sv_i - \frac{1}{|U|}\sum_{j\in U}\sv_j\bigg) = \sum_{i\in U}\sv_i - \sum_{j\in U}\sv_i\sum_{i\in U}\frac{1}{|U|}=\zero,\]
%which demonstrates that $\{\bgam_i\}$ are linearly dependent. Hence $\dim(\X)<|U|$. Now, suppose wlog that $U=[k]$ for some $k<n$. Since $\{\bgam_1,\dots,\bgam_k\}$ are linearly dependent, we have that $\spn(\{\bgam_1,\dots,\bgam_k\})=\spn(\{\bgam_1,\dots,\bgam_{k-1}\})$. Suppose now that $\bgam_1,\dots,\bgam_{k-1}$ are again dependent, and take $\alpha_1,\dots,\alpha_{k-1}\in \R$ such that 
%\[\zero = \sum_{i=1}^{k-1}\alpha_i\bgam_i = \sum_{i=1}^{k-1}\alpha _i \sv_i + c_U\sum_{i=1}^{k-1}\alpha_i = \sum_{i=1}^{k-1}\alpha_i\sv_i + \frac{\beta}{|U|}\sum_{i=1}^k \sv_i = \sum_{i=1}^{k-1}\sv_i\bigg(\alpha_i+\frac{\beta}{|U|}\bigg) + \frac{\beta}{|U|}\sv_k, \]
%where $\beta=\sum_{i=1}^{k-1}\alpha_i$. But this implies that $\{\sv_i\}_{i\in U}$ are linearly dependent, contradicting Lemma \ref{lem:svi_independent}. We now move onto the second assertion. Let $\vb{v}\in \X^\perp$, so $\v$ is orthogonal to every vector in $\X$. Therefore, for all $\alpha_1,\dots,\alpha_{k-1}\in \R$, 
%\begin{align}
%    0&=\bigg\la \vb{v},\sum_{i=1}^{k-1} \alpha_i\bgam_i\bigg\ra = \bigg\la \vb{v}, \sum_{i=1}^{k-1}\alpha_i\sv_i - \sum_{i=1}^{k-1}\frac{\alpha_i}{k}\sum_{j=1} ^{k}\sv_j\bigg\ra \notag \\
%    &= \bigg\la \vb{v},\sum_{i=1}^{k-1}\sv_i\bigg[\alpha_i -\sum_{j=1}^{k-1}\frac{\alpha_j}{k}\bigg] - \left(\sum_{i=1}^{k-1} \frac{\alpha_i}{k}\right)\sv_k\bigg\ra.\label{eq:lem_face_shift_1}
%\end{align}
%(Notice that the sum runs only until $k-1$ because of the discussion above.) Let $\p,\q\in \splx_{U}$, so $\p=\sum_{i\in U}x_i\sv_i=\sum_{i=1}^k x_i\sv_i $ for some $x_1,\dots,x_k\in\R$ with $\sum_i x_i=1$ and similarly, $\q=\sum_{i=1}^k y_i \sv_i$ with $\sum_iy_i=1$. The vector $\p-\q$ lies in $\splx_U$, so we must demonstrate that $\la \v,\p-\q\ra =\la \v,\sum_{i=1}^k \sv_i(x_i-y_i)\ra = 0$. By Equation \ref{eq:lem_face_shift_1} it suffices to demonstrate that we may choose $\alpha_1,\dots,\alpha_{k-1}\in \R$ such that $\alpha_i-\sum_{j=1}^{k-1}\alpha_j/k=(x_i-y_i)$ for all $i\in[k-1]$ and $-\sum_{i=1}^{k-1}\alpha_i/k=(x_k-y_k)$. Since this is a system of $k$ equations in $k-1$ unknowns, it is not obvious a priori that a solution exists. However, the final constraint is immediately satisfied by virtue of the fact that $\{x_i\}$ and $\{y_i\}$ are barycentric coordinates. Assuming the first $k-1$ constraints are satisfied (i.e., those for $x_i-y_i$, $i\in[k-1]$), observe 
%\begin{align*}    
%x_k-y_k &= 1 - \sum_{i=1}^{k-1}x_i -1 + \sum_{i=1}^{k-1}y_i = - \sum_{i=1}^{k-1}(x_i-y_i)  
%    = -\sum_{i=1}^{k-1} \bigg(\frac{k-1}{k}\alpha_i -\sum_{j=1,j\neq i}^{k-1}\frac{\alpha_j}{k}\bigg) \\
%    &= -\frac{k-1}{k}\sum_{i=1}^{k-1}\sum_{i=1}^{k-1}\alpha_i +\frac{1}{k} \sum_{i=1}^{k-1}\sum_{j=1,j\neq i}^{k-1}\alpha_j 
%    = -\frac{k-1}{k}\sum_{i=1}^{k-1}\sum_{i=1}^{k-1}\alpha_i +\frac{1}{k} \sum_{j=1 i}^{k-1}(k-2)\alpha_j \\
%    &= -\frac{1}{k}\sum_{i=1}^{k-1}\alpha_i.
%\end{align*}
%Therefore, it remains to show that the first $k-1$ constraints can be satisfied.
%Letting $\beta_i=x_i-y_i$ and $\bbeta=(\beta_1,\dots,\beta_{k-1})$ we can write the constraints succinctly as 
%\begin{equation*}
%    (\I_{k-1}-\frac{1}{k}\J_{k-1}^\tp )\balpha=\bbeta,
%\end{equation*}
%where the subscript $k-1$ signifies that the relevant matrices are square and of size $k-1$. To demonstrate that there exists some $\balpha$ which satisfies the above, it suffies to show that $\I_{k-1}-k^{-1}\J_{k-1}^\tp$ has rank $k-1$, or equivalently that its kernel has dimension zero. Let $\vp\in \ker(\I_{k-1}-k^{-1}\J_{k-1}^\tp)$. Then $\vp=k^{-1}\one\one^\tp \vp=(\norm{\vp}_1/k)\cdot\one$, implying that $\vp$ is a constant vector, say $\vp=a\cdot \one$. We require that 
%\[a\cdot\one = \frac{\norm{a\cdot \one}_1}{k}\one = \frac{a(k-1)}{k}\one,\]
%which is only satisfied for $a=0$. Hence $\vp=\zero$, and  $\ker(\I_{k-1}-k^{-1}\J_{k-1}^\tp)=\{0\}$ as desired. Consequently, we may choose $\alpha_1,\dots,\alpha_{k-1}$ to satisfy the constraints and finally conclude that $\la \vb{v},\p-\q\ra =0$. Therefore, $\X^\perp$ is a subset of all the 
%\end{proof}

\begin{corollary}
	\label{cor:dual_Sn}
	The dual simplex to $\splxn_G^+$ has vertices 
	\begin{equation*}
	\bigg\{\frac{\svn_i^+}{\beta_i + n\la \cent(\ssplx),\svn_i^+\ra}\bigg\},
	\end{equation*}
	where 
	\[\beta_i = \bigg(w(i)\sum_{j\neq i}w(j)\bigg)^{1/2}.\]
\end{corollary}
\begin{proof}
	Lemma \ref{lem:SUn_subset} and Equation \eqref{eq:Sn_altdesc} tells us that $\splxn = \bigcap_i \H_i^\geq $ where  $\H_i = \{\x:\la \x,\svn^+_i\ra \geq -\beta_i/n\}$. Lemma \ref{lem:hdesc_centred} then implies that 
	\begin{equation*}
	\splxn_0 = \bigcap_i \bigg\{\x:\la \x,\svn_i^+\ra \geq -\frac{\beta_i}{n} - \la \cent(\ssplx),\svn_i^+\ra\bigg\}.
	\end{equation*}
	Put $\kappa_i = -\beta_i/n - \la \cent(\ssplx),\svn_i^+\ra$. 
	Lemma \ref{lem:hdesc_dual} then dictates that the dual vertices of $\splx_0^D$ obey 
	\begin{equation*}
	\bigg\{\frac{\svn_i^+}{-n\cdot \kappa_i}\bigg\} = 	\bigg\{\frac{\svn_i^+}{\beta_i + n\la \cent(\ssplx),\svn_i^+\ra}\bigg\}.
	\end{equation*}
	Finally, since a centred simplex and its uncentred counterpart share the same dual simplex by Observation \ref{obs:dual_centred}, the result follows. 
\end{proof}





% Arithemetic argument that the normalized simplex and its inverse are not in fact duals
One might imagine (and could be forgiven for doing so) that once centred, the normalized simplex and its inverse would indeed be duals. However, this is not the case. Observe that 
\begin{align*}
\la \cent(\splx^+),\svn_j-\svn_k\ra &= \frac{1}{n}\sum_{\ell=1}^{n-1} \sum_{r=1}^n \svn^+_r(\ell)(\svn_j(\ell)-\svn_k(\ell))  \\&= 
\frac{1}{n}\sum_{r=1}^n \sum_{\ell=1}^{n-1} (\vp_\ell(r)\vp_\ell(j) - \vp_\ell(r)\vp_\ell(k) ) \\
&= \frac{1}{n} \sum_{r=1}^n \bigg(\delta_{r,k} - \delta_{r,j} + \frac{\sqrt{w(r)w(k)}}{n} - \frac{\sqrt{w(r)w(j)}}{n}\bigg) \\
&= \frac{\sqrt{w(k)}-\sqrt{w(j)}}{n^2} \sum_{r\in[n]}\sqrt{w(r)},
\end{align*}
so, by Equation \eqref{eq:lem_inverse_not_dual},
\begin{align*}
\la \svn_i^+-\cent(\splxn^+),(\svn_j-\cent(\splx)) - (\svn_k-\cent(\splx))\ra &= \la \svn_i^+,\svn_j-\svn_k\ra - \la \cent(\splxn^+),\svn_j-\svn_k\ra \\
&= \delta_{ij} - \delta_{ik} + \frac{\sqrt{w(i)}}{n}(\sqrt{w(k)} - \sqrt{w(j)})  \\
&\qquad - \frac{\sqrt{w(k)}-\sqrt{w(j)}}{n^2} \sum_{r\in[n]}\sqrt{w(r)} \\
&= \delta_{ij} + (\sqrt{w(k)}-\sqrt{w(j)})\bigg(\frac{\sqrt{w(i)}}{n} - \frac{1}{n^2}\sum_{r\in[n]}\sqrt{w(r)}\bigg),
\end{align*}
which is not equal to $\delta_{ij}$ unless $w(k)=w(j)$ or $\sqrt{w(i)}= \frac{1}{n}\sum_r \sqrt{w(r)}$. Since this would have to hold for all $i,j,k$, $i,j\neq k$, both of these conditions imply that the graph would have to be regular. \note{So what IS the dual of these simplices??  Ughgggghghgh wtf is happening}


\note{What is the following lemma used for?}
\begin{lemma}
	$\ker(\L^+)\subset\ker(\L)$ and $\ker(\Ln^+)\subset\ker(\Ln)$.  
\end{lemma}
\begin{proof}
	Let $\x\in\ker(\L^+)$, so $\L^+\x=\zero$. Multiplying by $\L$ and using Equation \eqref{eq:LL+} gives $\zero=\L\L^+\x=(\I-\one\one^\tp/n)\x$, implying that $\x=\one \cdot \norm{\x}_1/n$, i.e., $\x\in\spn(\{\one\})=\ker(\L)$. The argument for the other inclusion is similar.
\end{proof}


% Inverse graph stuff

%\section{The Inverse Graph}
%\label{sec:inverse_graph}
%Let $G$ be a connected and weighted graph. $G$ admits the hyperacute combinatorial simplex $\splx_G$ which, by Theorem \ref{thm:graph-simplex}, is the inverse simplex of a graph $H$. It thus obeys 
%\begin{equation*}
%\norm{\sv_i-\sv_j}_2^2 = \effr_H(i,j). 
%\end{equation*}
%%Expanding both sides for $i\neq j$ and  using the definition of the effective resistance yields 
%\begin{equation*}
%w_G(i) + w_G(j) + 2w_G(i,j) = \L_H^+(i,i) + \L_H^+(j,j) -2\L_H^+(i,j). 
%\end{equation*}
%Here we using the subscript $G$ to reinforce the fact that the weights are those of the original graph, $G$. We can calculate the entries of the pseudoinverse $\L_H^+$ more explicitly. Put 
%\[W_G\equiv \sum_{i<j}w(i,j) = \frac{1}{2}\sum_i \sum_j w(i,j) = \frac{1}{2}\sum_i w(i).\]
%That is, $W_G$ is the total weight of the graph $G$. Recall that $R_H$ is the total effective resistance of $H$ and compute 
%\begin{align*}
%R_H &= \sum_{i<j} \effr(i,j) \\
%&= \sum_{i<j} (w_G(i) + w_G(j) + 2w_G(i,j)) \\
%&= \frac{1}{2}\sum_{i,j} (w_G(i) +w_G(j)) + 2W_G \\
%&= (2n+2)W_G 
%\end{align*} 

Using this and a previous calculated formula for the entries of the pseudoinverse yields 
%\begin{align*}
\L_H^+(i,j) &= \frac{1}{2}\bigg(\sum_k \effr_H(i,k) + \sum_k \effr_H(j,k)\bigg) - \frac{1}{2} \effr_H(i,j) - \frac{R_H}{n^2}, \\
&= \frac{1}{2}\sum_k (w_G(i) + w_G(j) + 2w_G(k) + 2w_G(i,k) + 2w_G(j,k)) - \frac{1}{2}w_G(i,j) - \frac{R_H}{n^2}\\
&= \bigg(\frac{n}{2}+1\bigg)(w_G(i)+w_G(j))  - \frac{1}{2}w_G(i,j) + \bigg(2-\frac{2}{n}-\frac{2}{n^2}\bigg)W_G.
\end{align*}



