
%\begin{observation}
%\label{obs:p-q}
%Every vector parallel to the face $\splx_U$ is parallel to a vector of the form $\p-\q$ for $\p,\q\in\splx_U$. 
%\end{observation}



%\TODO Give intuition for what's about to happen. And give figures.
%
%\begin{lemma}
% $\spn(\{\sv_1-\sv_j\}_{j\in U})$ is parallel to $\splx_U$. 
%\end{lemma}
%Fix $i\in[n]$. By virtue of the fact that the vertex vectors are affinely independent, it follows that 
%\[\dim(\spn(\{\sv_1-\sv_j\}_{j\geq 2,j\neq i})=n-2,\]
%meaning the set of vectors $\{\sv_1-\sv_j\}_{j\neq i}$ forms a hyperplane which passes through the origin. Let $\w$ be the normal direction of this hyperplane, so that $\la \w,\v\ra=0$ for all \begin{equation}
%\label{eq:v_in_span}
%    \v=\sum_{j\geq 2,j\neq i}\alpha_j(\sv_1-\sv_j)\in\spn_\R(\{\sv_1-\sv_j\}_{j\geq 2,j\neq i})
%\end{equation} it holds that  $\la \w,\v\ra=0$. Let $\p,\q\in\splx_\ic$, so that $\p-\q$ is parallel so $\splx_\ic$. Let $\x$ and $\y$ be the barycentric coordinates of $\p$ and $\q$. Observe that 
%\begin{align*}
%    \p-\q&= \Sv(\x-\y) 
%    = \sum_{j\neq i} (x(i) - y(i))\sv_j \\
%    &= \bigg(1-\sum_{\ell}x(\ell) - 1 + \sum_\ell y(\ell)\bigg)\sv_1 + \sum_{j\geq 2;j\neq i}(x(j)-y(j))\sv_j \\
%    &= \sum_{j\geq 2,j\neq i} (y(j)-x(j))(\sv_1-\sv_j). 
%\end{align*}
%The inequality on the second line follows from the fact that $\x$ and $\y$ are barycentric coordinates and hence sum to one. Taking $\alpha_j = y(j)=x(j)$ in \eqref{eq:v_in_span} shows that $\la \w,\p-\q\ra =0$. Combined with Claim \ref{claim:p-q}, this demonstrates that $\spn_\R(\{\sv_1-\sv_j\}_{j\neq i})$ is parallel to $\splx_\ic$. 
%
%\begin{lemma}
%$\splx_U\subset \spn(\{\sv_1-\sv_j\}_{j\in U}) + \cent(\splx_U)$. 
%\end{lemma}
%
%For each $i\in[n]$ we therefore define the \emph{hyperplane associated with the facet $\splx_\ic$} as 
%\begin{equation}
%    \H_\ic \equiv \spn(\{\sv_1-\sv_j\}_{j\neq i,j>1}) + \cent(\splx_\ic).
%\end{equation}
%In other words, $\H_\ic$ is the affine space of dimension $n-2$ in which $\splx_\ic$ lies.
%
%\begin{lemma}
%For any proper subset $U\subsetneq[n]$, 
%$\dim(\spn_\R(\{\sv_j\}_{j\in U}))=|U|$. 
%\end{lemma}
%\begin{proof}
%It's immediate that 
%\[|U|\geq \dim(\spn(\{\sv_j\}_{j\in U})) \geq \dim(\spn_\R(\{\sv_1-\sv_j\}_{j\in U}))=|U|-1,\]
%by linear independence of the set $\{\sv_1-\sv_j\}$. Therefore, to demonstrate that the dimension is indeed $|U|$, it suffices to find a point $\x\in\spn(\{\sv_j\}_{j\in U})\setminus \spn_\R(\{\sv_1-\sv_j\}_{j\in U})$. 
%\end{proof}
%
%\note{Unclear where this stuff goes/whether it's needed}
%\begin{lemma}
%\label{lem:svi_independent}
%Let $G=(V,E)$ be a graph, and let $U\subsetneq V$ (i.e., $|U|<n)$. Then 
%\[\dim(\spn_\R(\{\sv_i\}_{i\in U}))=\dim(\spn_\R(\{\sv_i^+\}_{i\in U}))=|U|.\]
%\end{lemma}
%
%\begin{lemma}
%\label{lem:face_shift}
%Let $U\subsetneq V$. For each $i\in U$ set $\bgam_i = \sv_i - c_U$ where $c_U=c(\splx_U)$ is the centroid of the face $\splx_U$. Then 
%\[\dim(\spn_\R(\{\bgam_i\}_{i\in U}))=|U|-1,\]
%and every vector in $\spn(\{\bgam_i\}_{i\in U}))^\perp$, i.e., the orthogonal complement of $ \spn(\{\bgam_i\}_{i\in U}))$, is perpendicular to $\splx_{U}$. 
%\end{lemma}
%\begin{proof}
%For notational convenience, put $\X=\spn(\{\bgam_i\}_{i\in U})$. We begin by considering $\dim(\X)$. Notice that 
%\[\sum_{i\in U}\bgam_i = \sum_{i\in U}\bigg(\sv_i - \frac{1}{|U|}\sum_{j\in U}\sv_j\bigg) = \sum_{i\in U}\sv_i - \sum_{j\in U}\sv_i\sum_{i\in U}\frac{1}{|U|}=\zero,\]
%which demonstrates that $\{\bgam_i\}$ are linearly dependent. Hence $\dim(\X)<|U|$. Now, suppose wlog that $U=[k]$ for some $k<n$. Since $\{\bgam_1,\dots,\bgam_k\}$ are linearly dependent, we have that $\spn(\{\bgam_1,\dots,\bgam_k\})=\spn(\{\bgam_1,\dots,\bgam_{k-1}\})$. Suppose now that $\bgam_1,\dots,\bgam_{k-1}$ are again dependent, and take $\alpha_1,\dots,\alpha_{k-1}\in \R$ such that 
%\[\zero = \sum_{i=1}^{k-1}\alpha_i\bgam_i = \sum_{i=1}^{k-1}\alpha _i \sv_i + c_U\sum_{i=1}^{k-1}\alpha_i = \sum_{i=1}^{k-1}\alpha_i\sv_i + \frac{\beta}{|U|}\sum_{i=1}^k \sv_i = \sum_{i=1}^{k-1}\sv_i\bigg(\alpha_i+\frac{\beta}{|U|}\bigg) + \frac{\beta}{|U|}\sv_k, \]
%where $\beta=\sum_{i=1}^{k-1}\alpha_i$. But this implies that $\{\sv_i\}_{i\in U}$ are linearly dependent, contradicting Lemma \ref{lem:svi_independent}. We now move onto the second assertion. Let $\vb{v}\in \X^\perp$, so $\v$ is orthogonal to every vector in $\X$. Therefore, for all $\alpha_1,\dots,\alpha_{k-1}\in \R$, 
%\begin{align}
%    0&=\bigg\la \vb{v},\sum_{i=1}^{k-1} \alpha_i\bgam_i\bigg\ra = \bigg\la \vb{v}, \sum_{i=1}^{k-1}\alpha_i\sv_i - \sum_{i=1}^{k-1}\frac{\alpha_i}{k}\sum_{j=1} ^{k}\sv_j\bigg\ra \notag \\
%    &= \bigg\la \vb{v},\sum_{i=1}^{k-1}\sv_i\bigg[\alpha_i -\sum_{j=1}^{k-1}\frac{\alpha_j}{k}\bigg] - \left(\sum_{i=1}^{k-1} \frac{\alpha_i}{k}\right)\sv_k\bigg\ra.\label{eq:lem_face_shift_1}
%\end{align}
%(Notice that the sum runs only until $k-1$ because of the discussion above.) Let $\p,\q\in \splx_{U}$, so $\p=\sum_{i\in U}x_i\sv_i=\sum_{i=1}^k x_i\sv_i $ for some $x_1,\dots,x_k\in\R$ with $\sum_i x_i=1$ and similarly, $\q=\sum_{i=1}^k y_i \sv_i$ with $\sum_iy_i=1$. The vector $\p-\q$ lies in $\splx_U$, so we must demonstrate that $\la \v,\p-\q\ra =\la \v,\sum_{i=1}^k \sv_i(x_i-y_i)\ra = 0$. By Equation \ref{eq:lem_face_shift_1} it suffices to demonstrate that we may choose $\alpha_1,\dots,\alpha_{k-1}\in \R$ such that $\alpha_i-\sum_{j=1}^{k-1}\alpha_j/k=(x_i-y_i)$ for all $i\in[k-1]$ and $-\sum_{i=1}^{k-1}\alpha_i/k=(x_k-y_k)$. Since this is a system of $k$ equations in $k-1$ unknowns, it is not obvious a priori that a solution exists. However, the final constraint is immediately satisfied by virtue of the fact that $\{x_i\}$ and $\{y_i\}$ are barycentric coordinates. Assuming the first $k-1$ constraints are satisfied (i.e., those for $x_i-y_i$, $i\in[k-1]$), observe 
%\begin{align*}    
%x_k-y_k &= 1 - \sum_{i=1}^{k-1}x_i -1 + \sum_{i=1}^{k-1}y_i = - \sum_{i=1}^{k-1}(x_i-y_i)  
%    = -\sum_{i=1}^{k-1} \bigg(\frac{k-1}{k}\alpha_i -\sum_{j=1,j\neq i}^{k-1}\frac{\alpha_j}{k}\bigg) \\
%    &= -\frac{k-1}{k}\sum_{i=1}^{k-1}\sum_{i=1}^{k-1}\alpha_i +\frac{1}{k} \sum_{i=1}^{k-1}\sum_{j=1,j\neq i}^{k-1}\alpha_j 
%    = -\frac{k-1}{k}\sum_{i=1}^{k-1}\sum_{i=1}^{k-1}\alpha_i +\frac{1}{k} \sum_{j=1 i}^{k-1}(k-2)\alpha_j \\
%    &= -\frac{1}{k}\sum_{i=1}^{k-1}\alpha_i.
%\end{align*}
%Therefore, it remains to show that the first $k-1$ constraints can be satisfied.
%Letting $\beta_i=x_i-y_i$ and $\bbeta=(\beta_1,\dots,\beta_{k-1})$ we can write the constraints succinctly as 
%\begin{equation*}
%    (\I_{k-1}-\frac{1}{k}\J_{k-1}^\tp )\balpha=\bbeta,
%\end{equation*}
%where the subscript $k-1$ signifies that the relevant matrices are square and of size $k-1$. To demonstrate that there exists some $\balpha$ which satisfies the above, it suffies to show that $\I_{k-1}-k^{-1}\J_{k-1}^\tp$ has rank $k-1$, or equivalently that its kernel has dimension zero. Let $\vp\in \ker(\I_{k-1}-k^{-1}\J_{k-1}^\tp)$. Then $\vp=k^{-1}\one\one^\tp \vp=(\norm{\vp}_1/k)\cdot\one$, implying that $\vp$ is a constant vector, say $\vp=a\cdot \one$. We require that 
%\[a\cdot\one = \frac{\norm{a\cdot \one}_1}{k}\one = \frac{a(k-1)}{k}\one,\]
%which is only satisfied for $a=0$. Hence $\vp=\zero$, and  $\ker(\I_{k-1}-k^{-1}\J_{k-1}^\tp)=\{0\}$ as desired. Consequently, we may choose $\alpha_1,\dots,\alpha_{k-1}$ to satisfy the constraints and finally conclude that $\la \vb{v},\p-\q\ra =0$. Therefore, $\X^\perp$ is a subset of all the 
%\end{proof}

\begin{corollary}
	\label{cor:dual_Sn}
	The dual simplex to $\splxn_G^+$ has vertices 
	\begin{equation*}
	\bigg\{\frac{\svn_i^+}{\beta_i + n\la \cent(\ssplx),\svn_i^+\ra}\bigg\},
	\end{equation*}
	where 
	\[\beta_i = \bigg(w(i)\sum_{j\neq i}w(j)\bigg)^{1/2}.\]
\end{corollary}
\begin{proof}
	Lemma \ref{lem:SUn_subset} and Equation \eqref{eq:Sn_altdesc} tells us that $\splxn = \bigcap_i \H_i^\geq $ where  $\H_i = \{\x:\la \x,\svn^+_i\ra \geq -\beta_i/n\}$. Lemma \ref{lem:hdesc_centred} then implies that 
	\begin{equation*}
	\splxn_0 = \bigcap_i \bigg\{\x:\la \x,\svn_i^+\ra \geq -\frac{\beta_i}{n} - \la \cent(\ssplx),\svn_i^+\ra\bigg\}.
	\end{equation*}
	Put $\kappa_i = -\beta_i/n - \la \cent(\ssplx),\svn_i^+\ra$. 
	Lemma \ref{lem:hdesc_dual} then dictates that the dual vertices of $\splx_0^D$ obey 
	\begin{equation*}
	\bigg\{\frac{\svn_i^+}{-n\cdot \kappa_i}\bigg\} = 	\bigg\{\frac{\svn_i^+}{\beta_i + n\la \cent(\ssplx),\svn_i^+\ra}\bigg\}.
	\end{equation*}
	Finally, since a centred simplex and its uncentred counterpart share the same dual simplex by Observation \ref{obs:dual_centred}, the result follows. 
\end{proof}





% Arithemetic argument that the normalized simplex and its inverse are not in fact duals
One might imagine (and could be forgiven for doing so) that once centred, the normalized simplex and its inverse would indeed be duals. However, this is not the case. Observe that 
\begin{align*}
\la \cent(\splx^+),\svn_j-\svn_k\ra &= \frac{1}{n}\sum_{\ell=1}^{n-1} \sum_{r=1}^n \svn^+_r(\ell)(\svn_j(\ell)-\svn_k(\ell))  \\&= 
\frac{1}{n}\sum_{r=1}^n \sum_{\ell=1}^{n-1} (\vp_\ell(r)\vp_\ell(j) - \vp_\ell(r)\vp_\ell(k) ) \\
&= \frac{1}{n} \sum_{r=1}^n \bigg(\delta_{r,k} - \delta_{r,j} + \frac{\sqrt{w(r)w(k)}}{n} - \frac{\sqrt{w(r)w(j)}}{n}\bigg) \\
&= \frac{\sqrt{w(k)}-\sqrt{w(j)}}{n^2} \sum_{r\in[n]}\sqrt{w(r)},
\end{align*}
so, by Equation \eqref{eq:lem_inverse_not_dual},
\begin{align*}
\la \svn_i^+-\cent(\splxn^+),(\svn_j-\cent(\splx)) - (\svn_k-\cent(\splx))\ra &= \la \svn_i^+,\svn_j-\svn_k\ra - \la \cent(\splxn^+),\svn_j-\svn_k\ra \\
&= \delta_{ij} - \delta_{ik} + \frac{\sqrt{w(i)}}{n}(\sqrt{w(k)} - \sqrt{w(j)})  \\
&\qquad - \frac{\sqrt{w(k)}-\sqrt{w(j)}}{n^2} \sum_{r\in[n]}\sqrt{w(r)} \\
&= \delta_{ij} + (\sqrt{w(k)}-\sqrt{w(j)})\bigg(\frac{\sqrt{w(i)}}{n} - \frac{1}{n^2}\sum_{r\in[n]}\sqrt{w(r)}\bigg),
\end{align*}
which is not equal to $\delta_{ij}$ unless $w(k)=w(j)$ or $\sqrt{w(i)}= \frac{1}{n}\sum_r \sqrt{w(r)}$. Since this would have to hold for all $i,j,k$, $i,j\neq k$, both of these conditions imply that the graph would have to be regular. \note{So what IS the dual of these simplices??  Ughgggghghgh wtf is happening}


\note{What is the following lemma used for?}
\begin{lemma}
	$\ker(\L^+)\subset\ker(\L)$ and $\ker(\Ln^+)\subset\ker(\Ln)$.  
\end{lemma}
\begin{proof}
	Let $\x\in\ker(\L^+)$, so $\L^+\x=\zero$. Multiplying by $\L$ and using Equation \eqref{eq:LL+} gives $\zero=\L\L^+\x=(\I-\one\one^\tp/n)\x$, implying that $\x=\one \cdot \norm{\x}_1/n$, i.e., $\x\in\spn(\{\one\})=\ker(\L)$. The argument for the other inclusion is similar.
\end{proof}


% Inverse graph stuff

%\section{The Inverse Graph}
%\label{sec:inverse_graph}
%Let $G$ be a connected and weighted graph. $G$ admits the hyperacute combinatorial simplex $\splx_G$ which, by Theorem \ref{thm:graph-simplex}, is the inverse simplex of a graph $H$. It thus obeys 
%\begin{equation*}
%\norm{\sv_i-\sv_j}_2^2 = \effr_H(i,j). 
%\end{equation*}
%%Expanding both sides for $i\neq j$ and  using the definition of the effective resistance yields 
%\begin{equation*}
%w_G(i) + w_G(j) + 2w_G(i,j) = \L_H^+(i,i) + \L_H^+(j,j) -2\L_H^+(i,j). 
%\end{equation*}
%Here we using the subscript $G$ to reinforce the fact that the weights are those of the original graph, $G$. We can calculate the entries of the pseudoinverse $\L_H^+$ more explicitly. Put 
%\[W_G\equiv \sum_{i<j}w(i,j) = \frac{1}{2}\sum_i \sum_j w(i,j) = \frac{1}{2}\sum_i w(i).\]
%That is, $W_G$ is the total weight of the graph $G$. Recall that $R_H$ is the total effective resistance of $H$ and compute 
%\begin{align*}
%R_H &= \sum_{i<j} \effr(i,j) \\
%&= \sum_{i<j} (w_G(i) + w_G(j) + 2w_G(i,j)) \\
%&= \frac{1}{2}\sum_{i,j} (w_G(i) +w_G(j)) + 2W_G \\
%&= (2n+2)W_G 
%\end{align*} 

Using this and a previous calculated formula for the entries of the pseudoinverse yields 
%\begin{align*}
\L_H^+(i,j) &= \frac{1}{2}\bigg(\sum_k \effr_H(i,k) + \sum_k \effr_H(j,k)\bigg) - \frac{1}{2} \effr_H(i,j) - \frac{R_H}{n^2}, \\
&= \frac{1}{2}\sum_k (w_G(i) + w_G(j) + 2w_G(k) + 2w_G(i,k) + 2w_G(j,k)) - \frac{1}{2}w_G(i,j) - \frac{R_H}{n^2}\\
&= \bigg(\frac{n}{2}+1\bigg)(w_G(i)+w_G(j))  - \frac{1}{2}w_G(i,j) + \bigg(2-\frac{2}{n}-\frac{2}{n^2}\bigg)W_G.
\end{align*}


\note{Very unclear if there's anything interesting here. Mostly just contains Karel's thought on rws at the moment. Think about: \\
	
	(1) can we generate  a theory/answer questions regarding random walks in simplices using our knowledge of rws in graphs. \\
	
	(2)Straight lines are geodesics. If in the simplex the path created by a random walk is a straight line, is this telling us the random walk is as ``efficient'' as possible? Whereas those with curved lines are inefficient? Unclear how to formalized this /where to take it. 
	After  thinking about this a bit, probably not: It just says that the eigenvalues corresponding to  the vertices which contribute to the starting position are equal.
}






\subsection{mixing time}
The distribution $\bpi=(\pi_1,\dots,\pi_n)$ corresponds to a point in the simplex, namely $\p_\pi=\S\bpi$. It is thus natural to wonder whether this point tells us anything interesting about the dynamics of the walk. 

The \emph{variation distance} between two distributions $p_1$ and $p_2$ with finite state space $S$ is given by 
\[\norm{p_1-p_2}_V = \frac{1}{2}\sum_{s\in S} |p_1(s)-p_2(s)|.\]

\paragraph{Mixing Time.} Let $\p_i^t$ be the distribution over the set of vertices $V$ at time $t$ obtained by beginning the random walk at vertex $i$. Define 
\[\Delta(t) = \max_{i\in V}\norm{\p_i^t-\bpi}_V,\]
where $\norm{\cdot}_V$ is the variation distance. Given $\eps>0$ set 
\[\tau(\eps) = \min\{t:\Delta(t)\leq \eps\}.\]
We have 

We begin with a quantity related to random walks on the graph. Fix a graph $G=(V,E,w)$. Given $U\subset V$, the \emph{conductance of the cut $(U,U^c)$}---or simply the \emph{conductance of $U$}---is 
\begin{equation*}
\cond_G(U) \equiv \frac{w(\delta U)}{\min\{\vol(U),\vol(U^c)\}}.
\end{equation*}
The  conductance of a set, roughly speaking, measures how well connected the set is to its neighbours---higher conductance implies higher connectedness. We are interested in how  relatively  connected a set  is,  which is the reason for the normalization. The total \emph{conductance of $G$} is 
\begin{equation*}
\cond_G \equiv \min_{U\subset V}\cond_G(U). 
\end{equation*}
We have the following inequality: 
\[\theta(S)\geq \lambda_2\bigg(1-\frac{|S|}{|V|}\bigg)\geq \frac{\lambda_2}{2},\]
which yields 
\[\norm{\Sv \chi S}_2^2 \geq \frac{|S|}{2}\lambda_{n-1}.\]
We can relate the eigenvalues of $G$ to the geometry of $\splx$ via the relation $\Sv\Sv^\tp = \Eval$. Hence
\[\norm{\Sv \chi_S}_2^2 \geq \frac{|S|}{2} \Sv\Sv^\tp (n-1,n-1)\geq \frac{|S|}{2} \min_{i}\{(\Sv\Sv^\tp)(i,i):(\Sv\Sv^\tp)(i,i)\neq 0\} = \frac{|S|}{2}\min_{i=1}^{n-1} \norm{\Pi_i(\Sv)}_2^2.  \]


\begin{observation}
	Let $\p$ be any vector 
	If $\p$ is any vector pointing from $\splx_U$ to $\splx_{U^c}$ which has a non-empty intersection with both faces, then $\norm{\p}_2 \geq \norm{\alt(\splx_U)}_2$. 
\end{observation}
\begin{proof}
	Geometry. \note{Work this out}. 
\end{proof}

	\item Looking at the random walk of a graph as a path in the simplex didn't yield anything too interesting. What about the other way around? Beginning at a random point in the simplex, if we take a "random walk" (this would have to be defined appropriately -- we take a weighted step towards each vertex with some probability), we end up at some point that we know as a result of graph theory. We also know what governs how quickly we converge to this point, and when the path will be "straight". We know it's the sizes of the eigenvalues which govern the convergence; if we're simply given a hyperacute simplex, what do the eigenvalues represent? Can we translate this into a statement about the dynamics of the random walk in terms of the simplex only, and not the graph?
\item Can we define the "inverse/dual" graph of $G$ as follows: $G$ yields a simplex $\splx_G$ which is hyperacute. It is therefore the inverse simplex of  graph $G^+$. How are $G$ and $G^+$ related? \note{Tried this in Section \ref{sec:inverse_graph}. Unclear as of yet whether it's interesting. }
\item The projection matrix $Y(e,f)=b_e^\tp \L_G^+b_f\sqrt{w(e)w(f)}$ is symmetric with real eigenvalues (see \cite{vishnoi2013lx}). It thus yields a simplex. Maybe explore its properties. 
\item Can use inequalities obtained in the effective resistance literature to obtain inequalities which pertain to all hyperacute simplices. See e.g.,\cite{alev2017graph} 
\item Do low rank approximations of the gram matrix maintain any of the simplex properties? This yields a smaller representation of the graph ... what properties does this representation have?
\item Embedding approximate distance matrix. 


\subsection{Discrete Time Random Walks}
In a \emph{discrete time random walk (DSRW)} we envision a walker who jumps from vertex $i$ to vertex $j$ with probability proportional to $w(i,j)$. To this end, one defines the transition matrix 
\begin{equation*}
\T(i,j) = \frac{w(i,j)}{w(i)}=\frac{\A_G(i,j)}{\sum_{k\in\delta(i)} \A_G(i,k)}.
\end{equation*}
It's clear that $\sum_i \T(i,j)=1$. 
The probability that the walker is at node $i$ at time $t$ is the probability that that she was at node $j$ at time $t-1$ and transitioned to node $i$. Thus, 
\begin{equation*}
\pi_i(t)=\sum_{j}\pi_j(t-1)\T(i,j),
\end{equation*}
or, more succinctly, 
\begin{equation*}
\bpi(t) = \T\bpi(t-1).
\end{equation*}
The stationary distribution $\bpi(\infty)\equiv \lim_t \bpi(t)$  satisfies $\bpi(\infty)=\T\bpi(\infty)$,  which yields that 
The stationary distribution of such a walk is given by 
\[\pi_i = \frac{\sum_{j\in \delta(i)} w(i,j)}{\sum_{j,k\in V}w(i,j)},\]
which, for an undirected and unweighted graph simplifies to $\pi_i=\deg(i)/2|E|$. 


A \emph{Continuous Time Random Walk}~\cite{masuda2017random} satisfies the equation 
\begin{equation}
\label{eq:ctrw_dynamics}
\frac{d\bpi(t)}{dt}= -\bpi(t)^\tp\W^{-1} \L,
\end{equation}
hence 
\begin{equation*}
\bpi(t)^\tp = \bpi(0)^\tp \exp(-\W^{-1}\L t).
\end{equation*}
After converging to the stationary distribution there is, by definition, no change in the distribution. Therefore, $d\bpi(t)/dt=0$ and Equation \eqref{eq:ctrw_dynamics} reduces to 
$-\bpi(t)\W^{-1}\L =\zero$. Therefore, $\bpi(t)\W^{-1}$ is a left eigenfunction of $\L$ or equivalently, $ \W^{-1}\bpi$ is a right eigenfunction with corresponding eigenvalue zero. Hence, $\W^{-1}\bpi\in\spn\{\one\}$, i.e., $\bpi\in\spn\{\w\}$. Since $\norm{\bpi(\infty)}_1=1$, we see that 
\[\bpi(\infty) = \frac{\w}{\norm{\w}_1}.\]
In particular, the CTRW shares the same stationary distribution as the DTRW. 




This link between the volume of a simplex and $\Gamma_G$,  combined with the relationship between the distance matrix of a hyperacute simplex and the effective resistance of a graph allows us to derive new equations relating spanning trees with the effective resistances. For example: 

\note{WRONG}
\begin{lemma}
	\label{lem:vol_and_spanning_trees}
	Let $\splx^+$ be the inverse combinatorial simplex of the graph $G$.
	For any $U\subsetneq  V(G)$ with $|U|=d$,  the volumes of $\splx^+$ and $\splx^+_{U}$ are related  as 
	\begin{equation}
	\label{eq:vol(splx+)vol(splx+_U)}
	\vol^2(\splx^+_U) = \pm \frac{(-1)^{n+d}}{2^{n+d-2} [(d-1)!]^2[(n-1)!]^2 } \frac{\det(\D(\splx^+_{U^c}))}{\vol^2(\splx^+)},
	\end{equation} 
	while $\Gamma_{G}$ and $\Gamma_{G[U^c]}$ obey
	\begin{equation}
	\label{eq:Gamma_GGamma_GUc}
	\Gamma_{G[U]}\Gamma_G = \pm  \frac{2^{d+n-2}}{(-1)^{d+n}\det\Reff_{G[U^c]}}.
	\end{equation}
\end{lemma}
\begin{proof}
	Let $\Q$  be the Menger matrix associated to the simplex $\splx_G^+$. The Menger matrix associated with the simplex $\splx^+_U$ is then  $\Q[U+1,U+1]$, where we use the shorthand $Q+1=Q\cup\{1\}$. (The addition of the 1 comes from that fact that the Menger matrix still has $(0 \;\one^\tp)$ as a first row and $(0\; \one)^\tp$ as a first column.) 
	Lemma~\ref{lem:volume} then gives 
	\[\vol^2(\splx^+_{U}) = \frac{(-1)^d}{2^{d-1} [(d-1)!]^2 } \det\Q[U+1,U+1].\] 
	Using Sylvester's formula (Lemma~\ref{lem:sylvester}), write 
	\[\det\Q[U+1,U+1] = \pm \frac{\det\Q[(U+1)^c,(U+1)^c]}{\det\Q}.\]
	Now, notice that $\Q[(U+1)^c,(U+1)^c]$ is precisely the distance matrix associated to $\splx_{U^c}^+$, which we write as $\D(\Sv^+_{U^c})$.  Applying Lemma~\ref{lem:volume} to relate $\det\Q$ to $\vol(\splx^+)$ we obtain 
	\begin{equation*}
	\vol^2(\splx^+_U) = \pm \frac{(-1)^{n+d}}{2^{n+d-2} [(d-1)!]^2[(n-1)!]^2 } \frac{\det\D(\splx^+_{U^c})}{\vol^2(\splx^+)},
	\end{equation*} 
	which is \eqref{eq:vol(splx+)vol(splx+_U)}.
	Lemma~\ref{lem:S_G_basic_properties} then implies that distance matrix of $\splx^+{U^c}$ is precisely the  effective resistance matrix of $G[U^c]$. Hence  $\det\D(\splx^+_{U^c})=\det\Reff_{G[U^c]}$. Translating the volume expressions in the above equation into expressions involving $\Gamma_G$ and $\Gamma_{G[U]}$ by Equation~\ref{eq:vol(T)} gives \eqref{eq:Gamma_GGamma_GUc}.  
\end{proof}

\begin{remark}
	Since every hyperacute simplex is  the inverse combinatorial simplex of  some graph, Lemma~\ref{lem:vol_and_spanning_trees} applies to  all hyperacute simplices. 
\end{remark}

\note{Still working on this ---  unsure if there's anything here.}
Let us examine a specific low rank approximation proposed by Drineas and Mahoney~\cite{drineas2005approximating}, which finds low rank approximations to Gram matrices. We will give a brief overview of their method in general, and then elaborate on how it applies to our case in particular. Let $\M\in\R^{n\times m}$ be a gram matrix. Using the probability distribution $F(i) = \M(i,i)^2 / \tr(\M^2)$ sample $a\leq m$ columns of $\M$ independently at random and with replacement, where $a$ is some given parameter. Let $I\subset[n]$, $|I|\leq a$, be the indices of sampled columns. Let $\bC\in\R^{n\times a}$ be the matrix formed by these columns (that is, $\bC=\M(\cdot,I)$).  Let $\Q$ be the matrix $\M(I,I)\in\R^{a\times a}$, i.e., the submatrix of $M$ with entries corresponding to indices in $I$, and $\Q^+_k$  the optimal rank $k$-approximation to $\Q^+$, the pseudoinverse of $\Q$ (section \ref{sec:background_pseudoinverse}). The low rank approximation to $\M$ is then 
\begin{equation*}
\tM \equiv \bC\M(I,I)_k^+\bC^\tp.
\end{equation*}

\begin{theorem}[\cite{drineas2005approximating}]
	Let $\M$ be a gram matrix and let $\tM$ be as above. Let $\eps>0$,  $k\leq c\in\N$. If $c=\Omega(k/\eps^4)$, then 
	\begin{equation*}
	\norm{\M-\tM}_\kappa \leq \norm{\M-\M_k}_\kappa + \eps \tr(\M^2),
	\end{equation*}
	for $\kappa=2,F$. 
\end{theorem}

Let us analyze how this result translates to the case when $\M=\L_G$. Let $I$ and $\bC$ be as above. First we observe that $\L_G(I,I)$ is simply the Laplacian on the subgraph $G[I]$. Put $\tG = G[I]$. Performing an eigendecomposition, write 
\begin{equation*}
\L_\tG = \sum_{r=1}^{|I|} \mu_r \bnu_r\bnu_r^\tp,
\end{equation*}
for where $\mu_1\geq \mu_2\geq \dots \geq \mu_{|I|}=0$ and $\{\bnu_r\}$ are the eigenvalues and eigenvectors of $\L_\tG$, respectively. The results of Section \ref{sec:background_pseudoinverse} then dictate that 
\begin{equation*}
\L_\tG^+ = \sum_{r=1}^{|I|} \frac{1}{\mu_r} \bnu_r\bnu_r^\tp,
\end{equation*}
and so the best rank $k$ approximation to $\L_\tG$ is given by 
\begin{equation*}
\L_k \equiv (\L_\tG^+)_k = \sum_{r=1}^{k} \frac{1}{\mu_r} \bnu_r\bnu_r^\tp.
\end{equation*}
The approximation for $\L_G$ is thus given by $\tL=\bC\L_k\bC^\tp=\bC\L_k^{1/2}\L_k^{\tp/2}\bC^\tp = (\L_k^{\tp/2}\bC^\tp)^\tp \L_k^{\tp/2}\bC^\tp$. That is, we can view $\tL$ as the gram matrix of the vectors given by the columns of $\tSv =  (\L_k^{\tp/2}\bC^\tp)$. 

Let us examine $\tSv^\tp=\C\L_k^{\tp/2}$. First consider $\rank(\bC)$, which we claim is $|I|$. Suppose $\bC\f=\zero$, where $\f:I \to \R$. Extend $\f$ to $\hf:[n]\to\R$ by setting $\hf(u)=0$ for all $u\in [n]\setminus I$. Then 
\[(\L_G\hf)(k) = \sum_{i\in[n]}\L_G(k,i)\hf(i) = \sum_{i\in I}\L_G(k,i) \f(i) + \sum_{i\in[n]\setminus I} \L_G(k,i)\hf(i) = \sum_{i\in I}\bC(k,i) \f(i) = 0,\]
implying that $\L_G\hf=\zero$, so $\hf\in\spn(\one))$. However, as long as $|I|\neq[n]$, this is impossible since  $\hf([n]\setminus I)=\zero$. Therefore, so long as $c<n$, we have $\rank(\bC) = c$. 
We now claim that $\rank(\bC\L_k^{\tp/2}) = \rank(\L_k^{\tp/2})$, which is easier to prove in the abstract. 

\begin{lemma}
	Let $\bS:\R^n\to\R^\ell$, $T\in\R^m\to\R^n$ be linear maps with $\rank(\bS)=\ell$. Then $\rank(\bS\T) = \rank(\T)$. 
\end{lemma}
\begin{proof}
	If $\T\f=\zero$ then clearly $\bS\T\f=\zero$ so $\dim\ker(\T)\leq \dim\ker(\bS\T)$. On the other hand, if $\bS\T\f=\zero$ then because $\bS$ is full rank, $\T\f=\zero$ implying that $\dim \ker \T\ge \ker\bS\T$. 	By the rank nullity Theorem (e.g., ~\cite{axler1997linear}) $\rank(\bS\T) + \dim\ker \bS\T = n = \rank(\T) + \dim\ker\T$ from which the result follows immediately. 
\end{proof}

Taking $\bC=\bS$ and $\T=\L_k^{\tp/2}$ in the above lemma gives that $\rank(\bC\L_k^{\tp/2})=k$. Consequently, the vertex matrix $\tSv\in\R^{|I|\times n}$ contains $n$ vectors in $\R^{|I|}$. Moreover, 
\[\rank(\tL) = \rank(\tSv^\tp \tSv) = \rank(\tSv)=k,\]
meaning the $n$ vectors span a $k$-dimensional space. 

One might hope that the approximation matrix $\tL$ was a Laplacian, but this does not seem to be the case in general. While it is true that $\tL(i,i)\geq 0$ (by virtue of being a gram matrix) and that $\tL\one=\zero$ (this follows since $\bC^\tp\one =\zero$ because the rows of $\bC^\tp$ are columns and hence rows of $\L_G$). However, 
\[\tL(i,j) = \sum_{r,s=1}^c \bC(i,r)\bC(j,s)\L_k(r,s),\]
which does not look to be necessarily non-positive. 


\note{Still working on this content. Since $\Reff_G=n\sum_i\lambda_i^{-1}=n\tr(\Sv\Sv^\tp)$,
	facts/inequalities pertaining to the effective resistance can be translated to the simplex. }



Another representation comes via the \emph{incidence matrix} of $G$, $\B_G\in \R^{E\times V}$, defined as follows. Place an arbitrary orientation on the edges of $G$ (say, for example, $(i,j)$ is directed from $i$ to $j$ iff $i<j$), and for an edge $e$, let $e^-\in V$ denote the vertex at which $e$ begins, and $e^+$ the vertex at which it ends. Set 
\begin{equation}
\label{eq:B_G}
\B_G(e,i)=\begin{cases}
1&\text{if }i=e^-,\\
-1&\text{if }i=e^+,\\
0&\text{otherwise},
\end{cases}
\end{equation}
or, equivalently, $\B_G(e,i) = (\chi_{(i=e^-)}-\chi_{(i=e^+)})$. Then,
\begin{equation*}
( \B_G^\tp\W_G\B_G)(i,j)=\sum_{e\in E} \B_G^\tp(i,e)\B_G(e,j)=\sum_{e\in E}w(e)(\chi_{i=e^-}-\chi_{i=e^+})(\chi_{j=e^-}-\chi_{j=e^+}).
\end{equation*}
Let $\alpha(e)=(\chi_{i=e^-}-\chi_{i=e^+})(\chi_{j=e^-}-\chi_{j=e^+})$. If $i=j$, then $\alpha(e)=1$ iff $e$ is incident to $i$, and 0 otherwise. If $i\neq j$, then $\alpha(e)=1$ for $e=(i,j)$ and 0 otherwise, regardless of whether $i=e^-$ and $j=e^+$ or vice versa (this is what ensures that the orientation we chose for the edges is inconsequential). Consequently, 
\begin{align*}
(\B_G^\tp\W_G\B_G)(i,j) = 
\begin{cases}
\sum_{e\ni i} w(e),&\text{if } i=j,\\
-w((i,j)),&\text{otherwise}, 
\end{cases}
\end{align*}
which is precisely $\L_G(i,j)$. That is, we have 
\begin{equation}
\label{eq:L=BTB}
\L_G=(\W_G^{1/2}\B_G)^\tp(\W_G^{1/2}\B_G).
\end{equation}


Likewise, 
\begin{equation*}
\W_G^{-1/2}\Bn_G^\tp \W_G\Bn_G\W_G^{-1/2} =  \W_G^{-1/2}\L_G\W_G^{-1/2}=\Ln_G
\end{equation*}


First  we  note that the total effective resistance translates to the total pairwise distance in the simplex: 
\begin{align}
\Rtot_G = \frac{1}{2}\sum_{i,j\in[n]} \effr(i,j) = \frac{1}{2}\sum_{i,j\in[n]}\norm{\sv_i^+-\sv_j^+}_2^2. \label{eq:rtot_simplex}
\end{align}
This connection gives us a new way to prove certain identities. For example, 
\begin{align*}
\sum_{i,j\in[n]}\norm{\sv_i^+-\sv_j^+}_2^2 &= n\sum_{in[n]}\norm{\sv_i^+}_2^2 + n\sum_{j\in[n]}\norm{\sv_j^+}_2^2 - 2\sum_{i,j\in[n]}\la\sv_i^+,\sv_j^+\ra \\ 
&= n\sum_{i\in[n]}\L_G^+(i,i) + n\sum_{j\in[n]}\L_G^+(j,j) - 2\sum_{i\in[n]}\la \L_G(i,\cdot),\one\ra = 2n\Tr(\L_G^+),
\end{align*}
since $\L_G^+\one=\zero$. 
Hence, by \eqref{eq:rtot_simplex},
\begin{equation*}
\Rtot_G=n\Tr(L_G^+) = n\sum_{i\in[n]}\sum_{j<n} \frac{1}{\lambda_i}\vp_i(j)^2 =\sum_{j<n}\frac{1}{\lambda_j}\norm{\vp_j}_2^2 = \sum_{j<n}\frac{1}{\lambda_j}.
\end{equation*}
This is of course a well-known equation---originally produced by Klein  and Randi\'{c}~\cite{klein1993resistance}---but the interest resides in the fact that it can be derived via the graph-simplex correspondence.  


\section{Normalized Laplacian}
\note{Not worth fleshing out  until we have more content for this section.}

Noting that 
\begin{equation*}
\cent(\splxn) = \frac{1}{n}\bigg(\sum_{\ell=1}^n \svn_\ell(1),\dots,\sum_{\ell=1}^n \svn_\ell(n)\bigg)^\tp,
\end{equation*}
we see that the vertices of $\splxn_0$ have coordinates
\begin{equation*}
\svn_i(j) - \cent(\splxn)(j) = \vpn_j(i)\lambdan_j^{1/2} - \frac{1}{n}\sum_{\ell=1}^n \vpn_j(\ell)\lambdan_j^{1/2} = \lambdan_j^{1/2} \bigg(\vpn_j(i) - \frac{1}{n}\la \vpn_j,\one\ra \bigg).
\end{equation*}
Likewise, the vertices of $\splxn_0^+$ have coordinates 
\begin{equation*}
\svn_i^+(j) = \evaln_j^{-1/2}\bigg(\vpn_j(i) - \frac{1}{n}\la \vpn_j,\one\ra\bigg).
\end{equation*}

Let $\cent$ be the centroid of the centred normalized Laplacian. Noting that $(\cent,\cent,\dots,\cent)=\cent\one^\tp$, 
the Gram Matrix of $\splxn_0$ is 
\begin{align*}
(\Svn-\cent\one^\tp)^\tp(\Svn-\cent\one^\tp) &= \Svn^\tp\Svn - \Svn^\tp\cent\one^\tp - \one\cent^\tp\Svn + \one\cent^\tp\cent\one^\tp \\
&= \Ln_G - \frac{1}{n} \Svn^\tp\Svn\one\one^\tp - \frac{1}{n}\one\one^\tp\Svn^\tp\Svn + \frac{1}{n^2}\Svn^\tp\Svn\one\one^\tp \\
&= \Ln_G - \frac{1}{n}\Ln_G\J - \frac{1}{n} \J\Ln_G+ \frac{1}{n^2}\J\Ln_G\J.\\
\end{align*}

\note{What are the properties of this matrix? It has an eigenvector of $\one$ with eigenvalue 0, but it does not seem to be a Laplacian. }


